{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import psycopg2\n",
    "import inquirer\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from selenium import webdriver\n",
    "from collections import defaultdict\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class AURNScraper:\n",
    "    \"\"\" \n",
    "    This class will navigate the website to collect air quality monitoring site information \n",
    "    an individual or all sites. For a single site the information will be stored as a dictionary\n",
    "    and site images can be downloaded. For all sites the information will be saved as a .pkl\n",
    "    file. From this database sites can be selected by distance from user specified coordinates\n",
    "    to retrieve the air quality monitoring data for a specified year. Below is a step by step \n",
    "    example.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    Example 1 - Single Site: \n",
    "        AURN = AURNScraper()\n",
    "        AURN.single_site_info(site_name='Port Talbot Margam', download_imgs=True)\n",
    "    Example 2 - Multiple Sites:\n",
    "        Step 1 - Retrieve site information for all AURN sites to .pkl file:\n",
    "            AURN = AURNScraper()\n",
    "            AURN.all_sites_info()\n",
    "        Step 2 - Find sites within specified distance from specified coordinates:\n",
    "            sites_for_download = AURN.find_sites_by_distance(X=394366, Y=807397, \n",
    "                                distance_m=50000) \n",
    "        Step 3 - Download monitoring data for chosen sites:\n",
    "            AURN.download_monitoring_data(sites_for_download)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL address to the AURN website \n",
    "    \"\"\"\n",
    "    def __init__(self, url: str ='https://uk-air.defra.gov.uk/interactive-map'):\n",
    "        self.url = url\n",
    "        current_dir = os.getcwd()\n",
    "        self.new_dir = os.path.join(current_dir, r'monitoring_files')\n",
    "        if not os.path.exists(self.new_dir):\n",
    "            os.makedirs(self.new_dir)\n",
    "        chromeOptions = webdriver.ChromeOptions()\n",
    "        prefs = {\"download.default_directory\" : self.new_dir}\n",
    "        chromeOptions.add_experimental_option(\"prefs\",prefs)\n",
    "        chromeOptions.add_argument('--headless') # UNHASH WHEN COMPLETE\n",
    "        chromeOptions.add_argument('--disable-gpu') # UNHASH WHEN COMPLETE\n",
    "        self.driver = webdriver.Chrome(options=chromeOptions)\n",
    "        self.driver.get(url)\n",
    "\n",
    "# accept cookies\n",
    "    def _accept_cookies(self):\n",
    "        \"\"\" \n",
    "        This is a private method which accepts cookies when the webpage is initiated.\n",
    "        \"\"\"\n",
    "        WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.ID, 'global-cookie-message')))\n",
    "        cookie_window = self.driver.find_element_by_xpath(\"//div[@id='global-cookie-message']\")\n",
    "        cookie_window.find_element_by_xpath(\".//button[@name='submit']\").click()\n",
    "\n",
    "# find specified site\n",
    "    def single_site_info(self, site_name: str, download_imgs:bool = False) -> dict:\n",
    "        \"\"\" \n",
    "        Returns site information (Environment type, X and Y Coordinates, Location and URL link) \n",
    "        for a specified single site.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        site_name : str\n",
    "            site name as per the AURN website\n",
    "        \n",
    "        download_imgs : bool\n",
    "            Choose if you want to download pictures of this site. If set to True, images will be \n",
    "            downloaded to current directory. The default value is False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dictionary\n",
    "            {'Site': f'{site_name}', 'Site Info: ': {'Env_Type': '[env type]', 'X_and_Y': '[coordinates]', \n",
    "            'Location': '[location]', 'Web Link': f'{site_info_link}'}}\n",
    "        \"\"\"\n",
    "        site_info_dict = {'Name': [], 'Environment Type': [], 'Coordinates':[], 'Address':[], 'Web Link': [], 'Image Names':[]}\n",
    "        self._accept_cookies()\n",
    "        api_df = self._dataframe_API()\n",
    "        my_site = api_df.loc[api_df['site_name'] == site_name, 'site info link'].iloc[0]\n",
    "        site_info = self._retrieve_site_info(site_name, my_site, download_imgs)\n",
    "        site_info_dict['Name'].append(site_name)\n",
    "        site_info_dict['Environment Type'].append(site_info[0])\n",
    "        site_info_dict['Coordinates'].append(site_info[1])\n",
    "        site_info_dict['Address'].append(site_info[2])\n",
    "        site_info_dict['Web Link'].append(site_info[3])\n",
    "        site_info_dict['Image Names'].append(self._check_for_image_download(site_name))\n",
    "        print(site_info_dict)\n",
    "        return site_info_dict\n",
    "    \n",
    "    def _retrieve_site_info(self, site_name, this_site, retrieve_img=False):\n",
    "        \"\"\" \n",
    "        This is a private method which collates the site information for each site.\n",
    "        \"\"\"\n",
    "\n",
    "        self.driver.get(this_site)\n",
    "        #retrieve data in dictionary: Site Name, Location, Environment Type, eastings, northings, pollutants measured?\n",
    "        WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.ID, 'tab_info')))\n",
    "        tab_info = self.driver.find_element_by_xpath(\"//div[@id='tab_info']\")\n",
    "        print(tab_info)\n",
    "        my_tags = tab_info.find_elements_by_tag_name('p')\n",
    "        for info in my_tags:\n",
    "            if 'Environment Type' in info.text:\n",
    "                env_type = info.text.split(': ')[1]\n",
    "            elif 'Easting/Northing' in info.text:\n",
    "                try:\n",
    "                    X_co = int(info.text.split(': ')[1].split(', ')[0])\n",
    "                    Y_co = int(info.text.split(': ')[1].split(', ')[1])\n",
    "                    site_xy = [X_co, Y_co]\n",
    "                except:\n",
    "                    site_xy = info.text.split(': ')[1]\n",
    "            elif 'Site Address' in info.text:\n",
    "                site_address = info.text.split(': ')[1]\n",
    "        if retrieve_img == True:\n",
    "            self._retrieve_images(site_name)\n",
    "        #print(env_type, site_xy, site_address)\n",
    "        return [env_type, site_xy, site_address, this_site]\n",
    "    \n",
    "    def _retrieve_images(self, site_name, folder_name='image_files'):\n",
    "        \"\"\"\n",
    "        This is a private method which downloads images for a single site if the\n",
    "        user specifies to do this.\n",
    "        \"\"\"\n",
    "        current_dir = os.getcwd()\n",
    "        new_dir = os.path.join(current_dir, r'{f}'.format(f = folder_name))\n",
    "        if not os.path.exists(new_dir):\n",
    "            os.makedirs(new_dir)\n",
    "        site_photos = self.driver.find_element_by_xpath(\"//div[@class='carousel-inner']\")\n",
    "        all_photos = site_photos.find_elements_by_xpath(\"./div[@class='item']/*\")\n",
    "        filenumber = 0\n",
    "        for link in all_photos:\n",
    "            filename = f\"{site_name}{filenumber}\"\n",
    "            if os.path.exists(f'{new_dir}/{filename}.jpg')==True: # Do not download if image already exists\n",
    "                print(f\"{filename} already exists. Image not replaced.\")\n",
    "            else:\n",
    "                src = link.get_attribute('src')\n",
    "                urllib.request.urlretrieve(src,f\"{new_dir}/{filename}.jpg\")\n",
    "                filenumber += 1\n",
    "        print (\"Number of images downloaded: \", filenumber)\n",
    "        return \"Number of images downloaded: \", filenumber\n",
    "\n",
    "    def all_sites_info(self) -> dict:\n",
    "        \"\"\" \n",
    "        Returns site information (Environment type, X and Y Coordinates, Location and URL link) \n",
    "        for all sites on the AURN website.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        N/A\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dictionary\n",
    "            {'Site': f'{site_name}', 'Site Info: ': {'Env_Type': '[env type]', 'X_and_Y': '[coordinates]', \n",
    "            'Location': '[location]', 'Web Link': f'{site_info_link}'}}\n",
    "            \n",
    "            All_Sites_Output.pkl: Output file of all sites\n",
    "        \"\"\"\n",
    "        try: # If All_Sites_Ouput.pkl already exists ask user if they want to overwrite it\n",
    "            if os.path.isfile('All_Sites_Outputs.pkl') == True:\n",
    "                question = {inquirer.Confirm('confirmed',\n",
    "                    message=\"It looks like an output file with all the sites in has already been created. Do you want to overwrite this?\",\n",
    "                    default=True),}\n",
    "                ans = inquirer.prompt(question)\n",
    "            if ans['confirmed']==False:\n",
    "                print(\"Ok. Method has ended.\")\n",
    "                return\n",
    "        except: # Above question won't work unless file being called is a .py file\n",
    "            pass\n",
    "        self._accept_cookies()\n",
    "        site_info_dict = {'UUID': [],'Name': [], 'Environment Type': [], 'Coordinates':[], 'Address':[], 'Web Link': [], 'Image Names':[]}\n",
    "        api_df = self._dataframe_API()\n",
    "        for i in range(len(api_df)):\n",
    "            try:\n",
    "                name_of = api_df['site_name'][i]\n",
    "                print(type(name_of), name_of)\n",
    "                site_info_dict['UUID'].append(uuid.uuid4())\n",
    "                site_info_dict['Name'].append(name_of)\n",
    "                api_site_link = api_df['site info link'][i]\n",
    "                print(api_site_link)\n",
    "                site_info = self._retrieve_site_info(name_of, api_site_link)\n",
    "                print(site_info)\n",
    "                site_info_dict['Environment Type'].append(site_info[0])\n",
    "                site_info_dict['Coordinates'].append(site_info[1])\n",
    "                site_info_dict['Address'].append(site_info[2])\n",
    "                site_info_dict['Web Link'].append(site_info[3])\n",
    "                site_info_dict['Image Names'].append(self._check_for_image_download(name_of))\n",
    "                print(site_info_dict)\n",
    "            except IndexError:\n",
    "                print(\"Index Error\", i)\n",
    "                print (site_info_dict)\n",
    "                pass\n",
    "            except Exception as E:\n",
    "                print(\"Error\",E , i)\n",
    "                pass\n",
    "            finally:\n",
    "                site_info_df = pd.DataFrame.from_dict(site_info_dict)\n",
    "        self.driver.quit()    \n",
    "        site_info_df = pd.DataFrame.from_dict(site_info_dict)\n",
    "        site_info_df.to_pickle(\"All_Sites_Output.pkl\")\n",
    "        return site_info_df\n",
    "        \n",
    "    # find all sites within x distance\n",
    "    def find_sites_by_distance(self, X: float, Y: float, distance_m: int) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        This method will find all the sites within a specified distance of specified\n",
    "        X and Y coordinates.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        sites_for_download = AURN.find_sites_by_distance(X=394366, Y=807397, distance_m=50000)  \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : float\n",
    "            The X axis coordinate of your specified point.\n",
    "\n",
    "        Y : float\n",
    "            The Y axis coordinate of your specified point.\n",
    "\n",
    "        distance_m : int\n",
    "            The distance from your specified point from which site information will be obtained. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df : pd.DataFrame\n",
    "            A dataframe of all the sites within the specified distance of the specified \n",
    "            coordinates.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            all_sites_file = pd.read_pickle(r'All_Sites_Outputs.pkl')\n",
    "        except:\n",
    "            print(\"All_Sites_Ouputs.pkl not in current directory. Move this file to current directory or run 'all_sites_info' method to retrieve data\")\n",
    "            return\n",
    "        if 'X' not in all_sites_file and 'Y' not in all_sites_file:\n",
    "            all_sites_file.insert(2, 'X', 0)\n",
    "            all_sites_file.insert(3, 'Y', 0)\n",
    "            all_sites_file.insert(5, 'distance from point', 0)\n",
    "\n",
    "        all_sites_file['X'] = all_sites_file['Coordinates'].str[0]\n",
    "        all_sites_file['Y'] = all_sites_file['Coordinates'].str[1]\n",
    "        ans = ((((X-all_sites_file['X'])**2)+((Y-all_sites_file['Y'])**2))**0.5)\n",
    "        all_sites_file['distance from point'] = ans\n",
    "        df = all_sites_file[all_sites_file['distance from point']< distance_m]\n",
    "        print(f\"Below are all sites within {distance_m / 1000}km of specified points\")\n",
    "        print(df)\n",
    "        return df\n",
    "    \n",
    "    #download data\n",
    "    def download_monitoring_data(self, dataframe: pd.DataFrame, year: int):\n",
    "        \"\"\" \n",
    "        This will download All Hourly Pollutant Data for a specified year \n",
    "        from the AURN website for all sites in a specified dataframe.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataframe : pd.DataFrame\n",
    "            Dataframe of sites you wish to download data for. Format of this parameter\n",
    "            should match the output of the 'find_sites_by_distance' method.\n",
    "\n",
    "        year : int\n",
    "            The year you wish to download data for. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        download_report : dict\n",
    "            Dictionary of how many files were successfully downloaded and which (if any)\n",
    "            sites were unsuccessfully downloaded.\n",
    "            \n",
    "        Downloaded Data : .csv files\n",
    "            The downloaded data will appear in the current directory as individual csv \n",
    "            files.\n",
    "        \"\"\"\n",
    "        self._accept_cookies()\n",
    "        download_report = {'Successful Downloads Count': 0, 'Unsuccessful Download list': []}\n",
    "        for index, row in dataframe.iterrows():\n",
    "            self.driver.get(row['Web Link'])\n",
    "            WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='scrtabs-tab-scroll-arrow scrtabs-tab-scroll-arrow-left']\")))\n",
    "            path = self.driver.find_element_by_xpath(\"//ul[@class='nav nav-tabs nav-tabs-responsive']\")\n",
    "            networks = path.find_element_by_xpath(\"./li[@id='li_tab_networks']\")\n",
    "            tag_a = networks.find_element_by_tag_name(\"a\")\n",
    "            self.driver.execute_script(\"arguments[0].click();\", tag_a)\n",
    "            tab_networks = path.find_element_by_xpath(\"//div[@id='tab_networks']\")\n",
    "            formatted_data = tab_networks.find_element_by_link_text('Pre-Formatted Data Files')\n",
    "            self.driver.execute_script(\"arguments[0].click();\", formatted_data)\n",
    "            table = self.driver.find_elements_by_xpath(\"//div[@class='table-responsive']/*\")\n",
    "            element = table[0]\n",
    "            all_years = element.find_elements_by_tag_name('a')\n",
    "            downloads_before_loop = download_report['Successful Downloads Count']\n",
    "            for AURN_year, loop_no in zip(all_years, range(len(all_years))):\n",
    "                if AURN_year.text == str(year):\n",
    "                    linkname = AURN_year.get_attribute('href')\n",
    "                    split_list = linkname.split('/')\n",
    "                    name = split_list[len(split_list)-1].split('?')[0]\n",
    "                    if os.path.exists(f'{self.new_dir}/{name}') == True:\n",
    "                        print(f'{name} monitoring file already exists. File not replaced.')\n",
    "                    else:\n",
    "                        self.driver.execute_script(\"arguments[0].click();\", AURN_year)\n",
    "                        download_report['Successful Downloads Count'] += 1\n",
    "                if (loop_no+1) == len(all_years):\n",
    "                    if downloads_before_loop == download_report['Successful Downloads Count']:\n",
    "                        download_report['Unsuccessful Download list'].append(row['Name'])\n",
    "        print(download_report)\n",
    "        return download_report\n",
    "    \n",
    "    # convert pkl records to json file\n",
    "    def pkl_to_json(self, folder_name : str='json_files'):\n",
    "        ''' \n",
    "        This will convert individual records in the All_Sites_Outputs.pkl file into json records\n",
    "        and save these in the \"json files\" directory or create this directory if it doesn't \n",
    "        already exist.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder_name : str\n",
    "            Name of folder you wish to save json files in. The default is \"json files\"\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        json files : .json\n",
    "            Individual records from All_Sites_Outputs.pkl as .json.       \n",
    "        '''\n",
    "        current_dir = os.getcwd()\n",
    "        new_dir = os.path.join(current_dir, r'{f}'.format(f = folder_name))\n",
    "        if not os.path.exists(new_dir):\n",
    "            os.makedirs(new_dir)\n",
    "        \n",
    "        output_file = pd.read_pickle(r'All_Sites_Outputs.pkl')\n",
    "        for index, row in output_file.iterrows():\n",
    "            site_name = output_file.iloc[index]['Name']\n",
    "            output_file.iloc[index].to_json(r'{nd}/{si}.json'.format(nd = new_dir, si = site_name))\n",
    "        print(f'individual record of output file have been converted to json files in {new_dir}')\n",
    "    \n",
    "    def upload_directory_to_s3(self, folder : str, bucketname : str='airqualitywebscraperbucket'):\n",
    "        ''' \n",
    "        This method will upload the contents of your chosen folder to an AWS S3 bucket.\n",
    "        Chosen folder is likely to be either \"image_files\", \"json_files\", \"monitoring_files\".\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder : str\n",
    "            The name of the folder from which you wish to upload the contents to AWS S3 bucket.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Folder contents inputted to AWS S3 bucket. \n",
    "\n",
    "        '''\n",
    "        current_dir = os.getcwd()\n",
    "        path = os.path.join(current_dir,folder)\n",
    "        s3_client = boto3.client('s3')\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                s3_client.upload_file(os.path.join(root,file),bucketname,file)\n",
    "        return\n",
    "    \n",
    "    def _check_for_image_download(self, site_name : str) -> list:\n",
    "        current_dir = os.getcwd()\n",
    "        image_dir = os.path.join(current_dir, r'{f}'.format(f = 'image_files'))    \n",
    "        contin = True\n",
    "        val = 0\n",
    "        image_name_list = []\n",
    "        while contin == True:\n",
    "            if os.path.exists(f'{image_dir}/{site_name}{val}.jpg'):\n",
    "                image_name_list.append(f'{site_name}{val}')\n",
    "                val += 1\n",
    "            else:\n",
    "                contin = False\n",
    "        if len(image_name_list) == 0:\n",
    "            image_name_list.append(\"No Downloaded Images\")\n",
    "        return image_name_list\n",
    "\n",
    "    def _check_site_in_RDS(self, site_name):\n",
    "        connection = psycopg2.connect(user='postgres',\n",
    "                                password='mysecretpassword',\n",
    "                                host='airqualityscraper.clbqzprnzcak.eu-west-2.rds.amazonaws.com',\n",
    "                                port=5432,\n",
    "                                # server='scraper_data',\n",
    "                                database='postgres')\n",
    "        cursor = connection.cursor()\n",
    "        postgreSQL_select_Query = f\"\"\"Select \"Name\" FROM aq_data WHERE \"Name\" IN ('{site_name}');\"\"\"\n",
    "        cursor.execute(postgreSQL_select_Query)\n",
    "        my_val = cursor.fetchall()\n",
    "        if site_name in my_val[0]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # def _dataframe_API(self, API_loc=\"AURN_API.json\"):\n",
    "    #     json_file_path = API_loc\n",
    "    #     with open(json_file_path, 'r') as j:\n",
    "    #         contents = json.loads(j.read())\n",
    "    #     df = pd.DataFrame(contents['aurn'])\n",
    "    #     site_info_link = 'https://uk-air.defra.gov.uk/networks/site-info?uka_id='\n",
    "    #     df['site info link'] = site_info_link + df['uka_id']\n",
    "    #     return df\n",
    "    def _dataframe_API(self,API_loc=\"AURN_API.json\"):\n",
    "        '''\n",
    "        This is a private method that converts the json API with the\n",
    "        link to all AURN sites into a usable dataframe.\n",
    "        '''\n",
    "        json_file_path = API_loc\n",
    "        with open(json_file_path, 'r') as j:\n",
    "            contents = json.loads(j.read())\n",
    "        my_list = []\n",
    "        for i in contents.keys():\n",
    "            #print(f\"key: {i}\", len(contents[i]))\n",
    "            df = pd.DataFrame(contents[i])\n",
    "            #df.replace(\"\", numpy.NaN, inplace=True)\n",
    "            my_list.append(df)\n",
    "        df = pd.concat(my_list, axis=0)\n",
    "        df.drop(columns=['exception','parameter_ids',\n",
    "                'network_name', 'network_id','site_status',\n",
    "                'overall_index','environment_id','country_id']\n",
    "                ,axis=0, inplace=True)\n",
    "        site_info_link = 'https://uk-air.defra.gov.uk/networks/site-info?uka_id='\n",
    "        df['site info link'] = site_info_link + df['uka_id']\n",
    "        df = df[['site_name', 'site info link']]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> 0                       Aberdeen Erroll Park\n",
      "0    Anglesey Penhesgyn 3 (Isle of Anglesey)\n",
      "0                           Auchencorth Moss\n",
      "0                            Barnsley Gawber\n",
      "0                           Auchencorth Moss\n",
      "0                           Auchencorth Moss\n",
      "0                           Auchencorth Moss\n",
      "0                           Auchencorth Moss\n",
      "0                           Auchencorth Moss\n",
      "0                                    Belfast\n",
      "0                                  Goonhilly\n",
      "0                           Auchencorth Moss\n",
      "0                                  Goonhilly\n",
      "0                   Ainsdale Dunes and Sands\n",
      "0                           Auchencorth Moss\n",
      "0             Aberdeen Union Street Roadside\n",
      "Name: site_name, dtype: object\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "0    https://uk-air.defra.gov.uk/networks/site-info...\n",
      "Name: site info link, dtype: object\n",
      "Error Object of type Series is not JSON serializable 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7363/3787178881.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#AURN._dataframe_API()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#AURN.single_site_info('Sheffield Barnsley Road',download_imgs=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mAURN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_sites_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7363/656445175.py\u001b[0m in \u001b[0;36mall_sites_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0msite_info_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite_info_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0msite_info_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite_info_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/AQ_Webscraper/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"only recognize index or columns for orient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m     def to_numpy(\n",
      "\u001b[0;32m~/miniconda3/envs/AQ_Webscraper/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/AQ_Webscraper/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     return arrays_to_mgr(\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     )\n",
      "\u001b[0;32m~/miniconda3/envs/AQ_Webscraper/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/AQ_Webscraper/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "AURN = AURNScraper()\n",
    "#AURN._dataframe_API()\n",
    "#AURN.single_site_info('Sheffield Barnsley Road',download_imgs=False)\n",
    "AURN.all_sites_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': ['Port Talbot Margam'], 'Environment Type': ['Urban Industrial'], 'Coordinates': [[277406, 188719]], 'Address': ['Port Talbot'], 'Web Link': ['https://uk-air.defra.gov.uk/networks/site-info?uka_id=UKA00501']}\n"
     ]
    }
   ],
   "source": [
    "print(my_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(my_site['Coordinates'][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n        Returns site information (Environment type, X and Y Coordinates, Location and URL link) \\n        for a specified single site.\\n\\n        Parameters\\n        ----------\\n        site_name : str\\n            site name as per the AURN website\\n\\n        Returns\\n        -------\\n        Dictionary\\n            {'Site': f'{site_name}', 'Site Info: ': {'Env_Type': '[env type]', 'X_and_Y': '[coordinates]', \\n            'Location': '[location]', 'Web Link': f'{site_info_link}'}}\\n        \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AURN = AURNScraper('https://uk-air.defra.gov.uk/interactive-map')\n",
    "AURNScraper.single_site_info.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "AURN = AURNScraper('https://uk-air.defra.gov.uk/interactive-map')\n",
    "my_df = AURN.all_sites_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are all sites within 50.0km of specified points\n",
      "                              Name  Environment Type       X       Y  \\\n",
      "0             Aberdeen Erroll Park  Urban Background  394366  807397   \n",
      "19  Aberdeen Union Street Roadside     Urban Traffic  393656  805968   \n",
      "20        Aberdeen Wellington Road     Urban Traffic  394397  804779   \n",
      "\n",
      "         Coordinates  distance from point        Address  \\\n",
      "0   [394366, 807397]             0.000000  Not available   \n",
      "19  [393656, 805968]          1595.663185       Aberdeen   \n",
      "20  [394397, 804779]          2618.183531       Aberdeen   \n",
      "\n",
      "                                             Web Link  \n",
      "0   https://uk-air.defra.gov.uk/networks/site-info...  \n",
      "19  https://uk-air.defra.gov.uk/networks/site-info...  \n",
      "20  https://uk-air.defra.gov.uk/networks/site-info...  \n"
     ]
    }
   ],
   "source": [
    "AURN = AURNScraper()\n",
    "sites_for_download = AURN.find_sites_by_distance(X=394366, Y=807397, distance_m=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[394366, 807397]\n",
      "[393656, 805968]\n",
      "[394397, 804779]\n"
     ]
    }
   ],
   "source": [
    "for index, row in sites_for_download.iterrows():\n",
    "    print(row['Coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[394366, 807397]\n"
     ]
    }
   ],
   "source": [
    "print(sites_for_download.iloc[0]['Coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sucessful Downloads Count': 2,\n",
       " 'Unsuccessful Download list': ['Aberdeen Erroll Park']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AURN = AURNScraper('https://uk-air.defra.gov.uk/interactive-map')\n",
    "AURN.download_monitoring_data(sites_for_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Environment Type</th>\n",
       "      <th>Coordinates</th>\n",
       "      <th>Address</th>\n",
       "      <th>Web Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aberdeen Erroll Park</td>\n",
       "      <td>Urban Background</td>\n",
       "      <td>[394366, 807397]</td>\n",
       "      <td>Not available</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aston Hill</td>\n",
       "      <td>Rural Background</td>\n",
       "      <td>[329899, 290053]</td>\n",
       "      <td>Aston Hill</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Auchencorth Moss</td>\n",
       "      <td>Rural Background</td>\n",
       "      <td>[322166, 656128]</td>\n",
       "      <td>Auchencorth</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blackpool Marton</td>\n",
       "      <td>Urban Background</td>\n",
       "      <td>[333768, 434759]</td>\n",
       "      <td>Blackpool</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bush Estate</td>\n",
       "      <td>Rural Background</td>\n",
       "      <td>[324629, 663891]</td>\n",
       "      <td>Bush Estate</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Name  Environment Type       Coordinates        Address  \\\n",
       "0  Aberdeen Erroll Park  Urban Background  [394366, 807397]  Not available   \n",
       "1            Aston Hill  Rural Background  [329899, 290053]     Aston Hill   \n",
       "2      Auchencorth Moss  Rural Background  [322166, 656128]    Auchencorth   \n",
       "3      Blackpool Marton  Urban Background  [333768, 434759]      Blackpool   \n",
       "4           Bush Estate  Rural Background  [324629, 663891]    Bush Estate   \n",
       "\n",
       "                                            Web Link  \n",
       "0  https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "1  https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "2  https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "3  https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "4  https://uk-air.defra.gov.uk/networks/site-info...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "object = pd.read_pickle(r'All_Sites_Outputs.pkl')\n",
    "object.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[?] It looks like an output file with all the sites in has already been cre...: \n",
      "just run\n",
      "and keep running\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import inquirer\n",
    "try:\n",
    "    if os.path.isfile('All_Sites_Outputs.pkl') == True:\n",
    "        question = {inquirer.Confirm('confirmed',\n",
    "                                    message=\"It looks like an output file with all the sites in has already been created. Do you want to overwrite this?\",\n",
    "                                    default=True),}\n",
    "        ans = inquirer.prompt(question)\n",
    "    if ans['confirmed']==False:\n",
    "        quit()\n",
    "    print(\"Good job!\")\n",
    "except:\n",
    "    print(\"just run\")\n",
    "    pass\n",
    "print(\"and keep running\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n",
      "4\n",
      "https://uk-air.defra.gov.uk/assets/site-photos/BPLE_n.jpg\n",
      "https://uk-air.defra.gov.uk/assets/site-photos/BPLE_e.jpg\n",
      "https://uk-air.defra.gov.uk/assets/site-photos/BPLE_s.jpg\n",
      "https://uk-air.defra.gov.uk/assets/site-photos/BPLE_w.jpg\n"
     ]
    }
   ],
   "source": [
    "# Download images of site\n",
    "import urllib.request\n",
    "url = 'https://uk-air.defra.gov.uk/interactive-map'\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//div[@style='position: absolute; left: 0px; top: 0px; z-index: 106; width: 100%;']\")))\n",
    "my_map = driver.find_elements_by_xpath(\"//div[@style='position: absolute; left: 0px; top: 0px; z-index: 106; width: 100%;']/*\")\n",
    "site_info_dict = {'name': [], 'Environment Type': [], 'Coordinates':[], 'Address':[], 'Web Link': []}\n",
    "print(len(my_map))\n",
    "link = my_map[0]\n",
    "driver.execute_script(\"arguments[0].click();\", link)\n",
    "WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'popupContent')))\n",
    "popup = driver.find_element_by_xpath(\"//div[@id='popupContent']\")\n",
    "my_tags = popup.find_elements_by_tag_name('a')\n",
    "for link in my_tags:\n",
    "    if link.text == 'Site Information':\n",
    "        site_info_link = link.get_attribute('href')\n",
    "        #print(site_info_link)\n",
    "driver.get(site_info_link)\n",
    "\n",
    "#### images\n",
    "site_photos = driver.find_element_by_xpath(\"//div[@class='carousel-inner']\")\n",
    "all_photos = site_photos.find_elements_by_xpath(\"./div[@class='item']/*\")\n",
    "print(len(all_photos))\n",
    "filename = 1\n",
    "for link in all_photos:\n",
    "    src = link.get_attribute('src')\n",
    "    print(src)\n",
    "    urllib.request.urlretrieve(src,f\"{filename}.jpg\")\n",
    "    filename += 1\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memoization of dictionary atrtibutes\n",
    "\n",
    "def download_all_sites_memo(memo=None):\n",
    "    if memo is None:\n",
    "        memo = {'Name': [], 'Environment Type': [], 'Coordinates':[], 'Address':[], 'Web Link': []}\n",
    "    \n",
    "    link = my_map2[i]\n",
    "    self.driver.execute_script(\"arguments[0].click();\", link) # This is the updated link.click() to overcome spatial error on map with click\n",
    "    name_of = link.get_attribute('title')  \n",
    "    if memo['Name'] == name_of:\n",
    "        print(\"This name is already in dictionary\")\n",
    "    else:    \n",
    "        site_info_dict['Name'].append(name_of)\n",
    "        site_info = self._retrieve_site_info2(name_of, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "object = pd.read_pickle(r'All_Sites_Outputs.pkl')\n",
    "len(object) #.head()\n",
    "# dict(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0\n",
      "row Name                                             Aberdeen Erroll Park\n",
      "Environment Type                                     Urban Background\n",
      "Coordinates                                          [394366, 807397]\n",
      "Address                                                 Not available\n",
      "Web Link            https://uk-air.defra.gov.uk/networks/site-info...\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ogkjpiyb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4825/1854157354.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0msite_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mobject1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'{nd}/{si}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mogkjpiyb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ogkjpiyb' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "object1 = pd.read_pickle(r'All_Sites_Outputs.pkl')\n",
    "folder_name = 'json files'\n",
    "\n",
    "# site_name = object.iloc[0]['Name']\n",
    "# result = object.iloc[0].to_json(r'site_name')\n",
    "# parsed = json.loads(result)\n",
    "# json.dumps(parsed)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "new_dir = os.path.join(current_dir, r'{f}'.format(f = folder_name))\n",
    "if not os.path.exists(new_dir):\n",
    "    os.makedirs(new_dir)\n",
    "\n",
    "for index, row in object1.iterrows():\n",
    "    print (\"index\", index)\n",
    "    print (\"row\",  row)\n",
    "    site_name = object1.iloc[index]['Name']\n",
    "    object1.iloc[index].to_json(r'{nd}/{si}'.format(nd = new_dir, si = site_name))\n",
    "    ;ogkjpiyb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "boto3.client('s3').upload\n",
    "\n",
    "response = s3_client.upload_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uploadDirectory(folder : str, bucketname : str='airqualitywebscraperbucket'):\n",
    "    s3_client = boto3.client('s3')\n",
    "    for root,dirs,files in os.walk(path):\n",
    "        for file in files:\n",
    "            s3_client.upload_file(os.path.join(root,file),bucketname,file)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAL4_2022.csv\n"
     ]
    }
   ],
   "source": [
    "filename = 'https://uk-air.defra.gov.uk/data_files/site_data/WAL4_2022.csv?v=1'\n",
    "val = filename.split('/')\n",
    "#my_len = len(val)\n",
    "name = val[len(val)-1]\n",
    "name = name.split('?')\n",
    "name = name[0]\n",
    "print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAL4_2022.csv\n"
     ]
    }
   ],
   "source": [
    "split_list = filename.split('/')\n",
    "name = split_list[len(split_list)-1].split('?')[0]\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_name</th>\n",
       "      <th>site info link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aberdeen Erroll Park</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aberdeen Union Street Roadside</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aberdeen Wellington Road</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Armagh Roadside</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aston Hill</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>UUNN_WELW_002</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>UUNN_WIGA_001</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>UUNN_WIGA_002</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>UUNN_WIGA_003</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>Widnes Milton Road</td>\n",
       "      <td>https://uk-air.defra.gov.uk/networks/site-info...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1234 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          site_name  \\\n",
       "0              Aberdeen Erroll Park   \n",
       "1    Aberdeen Union Street Roadside   \n",
       "2          Aberdeen Wellington Road   \n",
       "3                   Armagh Roadside   \n",
       "4                        Aston Hill   \n",
       "..                              ...   \n",
       "301                   UUNN_WELW_002   \n",
       "302                   UUNN_WIGA_001   \n",
       "303                   UUNN_WIGA_002   \n",
       "304                   UUNN_WIGA_003   \n",
       "305              Widnes Milton Road   \n",
       "\n",
       "                                        site info link  \n",
       "0    https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "1    https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "2    https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "3    https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "4    https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "..                                                 ...  \n",
       "301  https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "302  https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "303  https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "304  https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "305  https://uk-air.defra.gov.uk/networks/site-info...  \n",
       "\n",
       "[1234 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy\n",
    "import pandas as pd\n",
    "# json_file_path = \"AURN_API.json\"\n",
    "# with open(json_file_path, 'r') as j:\n",
    "#      contents = json.loads(j.read())\n",
    "# #print(contents.keys())\n",
    "# #my_list = []\n",
    "# df1 = pd.DataFrame(contents['aurn'])\n",
    "# df2 = pd.DataFrame(contents['nondefraaqmon'])\n",
    "\n",
    "\n",
    "# df = pd.concat([df1,df2])\n",
    "# site_info_link = 'https://uk-air.defra.gov.uk/networks/site-info?uka_id='\n",
    "# df['site info link'] = site_info_link + df['uka_id']\n",
    "# print(df)\n",
    "# df.head()\n",
    "# len(df)\n",
    "\n",
    "def _dataframe_API(API_loc=\"AURN_API.json\"):\n",
    "     '''\n",
    "     This is a private method that converts the json API with the\n",
    "     link to all AURN sites into a usable dataframe.\n",
    "     '''\n",
    "     json_file_path = API_loc\n",
    "     with open(json_file_path, 'r') as j:\n",
    "          contents = json.loads(j.read())\n",
    "     my_list = []\n",
    "     for i in contents.keys():\n",
    "          #print(f\"key: {i}\", len(contents[i]))\n",
    "          df = pd.DataFrame(contents[i])\n",
    "          #df.replace(\"\", numpy.NaN, inplace=True)\n",
    "          my_list.append(df)\n",
    "     df = pd.concat(my_list, axis=0)\n",
    "     df.drop(columns=['exception','parameter_ids',\n",
    "            'network_name', 'network_id','site_status',\n",
    "            'overall_index','environment_id','country_id']\n",
    "            ,axis=0, inplace=True)\n",
    "     site_info_link = 'https://uk-air.defra.gov.uk/networks/site-info?uka_id='\n",
    "     df['site info link'] = site_info_link + df['uka_id']\n",
    "     df = df[['site_name', 'site info link']]\n",
    "     # df.to_csv()\n",
    "\n",
    "     # df = dict(df)\n",
    "     # for key, value in df.items(): \n",
    "     #      print(key, len(value), sep=\" | \")\n",
    "     return df\n",
    "\n",
    "_dataframe_API()\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import json\n",
    "# json_file_path = \"AURN_API.json\"\n",
    "# with open(json_file_path, 'r') as j:\n",
    "#      contents = json.loads(j.read())\n",
    "\n",
    "# my_list = []\n",
    "# for i in contents.keys():\n",
    "#     df= pd.DataFrame(contents[i])\n",
    "#     my_list.append(df)\n",
    "\n",
    "# big_df = pd.concat(my_list)\n",
    "# # big_df.replace(r'^\\s*$',np.nan,regex=True,inplace=True)\n",
    "# big_df.drop(columns=['exception','parameter_ids',\n",
    "#             'network_name', 'network_id','site_status',\n",
    "#             'overall_index','environment_id','country_id']\n",
    "#             ,axis=0, inplace=True) #,'parameter_ids','network_name'])\n",
    "# #df = pd.DataFrame(contents['aurn'])\n",
    "# big_df.to_csv('big_df.csv')\n",
    "\n",
    "\n",
    "# my_site = df['site info link'].loc[df['site_name']=='Sheffield Barnsley Road']\n",
    "# #type(my_site[0])\n",
    "# my_site = df.loc[df['site_name']=='Sheffield Barnsley Road'].index\n",
    "# df['site info link'].iloc[my_site]\n",
    "# my_s = dict(df['site info link'].iloc[my_site])\n",
    "# my_s[str(my_site)]\n",
    "\n",
    "#df.loc[df['site_name'] == 'Sheffield Barnsley Road', 'site info link'].iloc[0]\n",
    "\n",
    "# df['site_name'].iloc[0]\n",
    "# len(df)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1234\n"
     ]
    }
   ],
   "source": [
    "data = len(_dataframe_API()['network_name'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://uk-air.defra.gov.uk/js/map_data.php?c=833f31fa9e72923ac4402e26f4afd5572460ad92'\n",
    "response = requests.get(url)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
